{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "265d9e65",
   "metadata": {},
   "source": [
    "# Definition by sound example\n",
    "\n",
    "The goal of this section is to orient everyone towards the general concepts\n",
    "involved in the estimation of tempo, beat, and downbeats from musical\n",
    "audio signals.\n",
    "\n",
    "While this could be acheived through an extensive literature review\n",
    "supported by the diverse multi-disciplinary perspectives\n",
    "including music theory, psychology, and cognitive neuroscience,\n",
    "we instead try to provide a quick and intuitive means to grasp the relevant\n",
    "concepts via a set of three annotated sound examples.\n",
    "Where appropriate we draw upon the relevant different multi-disciplinary\n",
    "perspectives, as a means to demonstrate the interestingness\n",
    "of the tasks and the potential challenges when \n",
    "addressing them in a computational manner. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbe48a9",
   "metadata": {},
   "source": [
    "## A straightforward example\n",
    "\n",
    "Let's start with a relatively easy example which should help\n",
    "clarify what the intuitive target is for a system that\n",
    "can estimate the metrical structure of a piece of\n",
    "music. For this example we can plot the waveform and listen to the audio excerpt. \n",
    "\n",
    "```{admonition} Task\n",
    "Why not go ahead\n",
    "and try to tap the beat yourself? \n",
    "You can also try to count along as well.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0226900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do import and setup \n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "\n",
    "SR = 44100\n",
    "FIGSIZE = (14,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce95f5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x1, sr = librosa.load('../assets/ch2_basics/audio/easy_example.flac', sr = SR)\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "librosa.display.waveplot(x1, sr=SR, alpha=0.6); # this semi-colon surpresses the matplotlib stdout <matplotlib.zzz.zzz at 0x etc.>\n",
    "plt.title('Easy Example: audio waveform', fontsize=15)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.xlabel('Time', fontsize=13)\n",
    "plt.xlim(0, len(x1)/sr);\n",
    "ipd.Audio(x1, rate=SR) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e162d98",
   "metadata": {},
   "source": [
    "```{admonition} Wait! What is that amazing song? \n",
    ":class: dropdown\n",
    "[\"Shampoo\" from the album \"Shampoo\" by Yuko Tomita](https://www.discogs.com/release/8509227-Yuko-Tomita-Shampoo)\n",
    "\n",
    "At the time of writing, you can listen to the full version on [YouTube](https://www.youtube.com/watch?v=yJImRetSSu4)\n",
    "\n",
    "![](../assets/ch2_basics/figs/shampoo.jpg)\n",
    "```\n",
    "\n",
    "What we're trying to work towards in this tutorial\n",
    "is to design and execute a deep learning based\n",
    "system that can \"listen\" to pieces of music like this\n",
    "and then not only tap along to mark the beat,\n",
    "but also count in time to music.\n",
    "\n",
    "Essentially this means that it can discover\n",
    "a quasi-periodic sequence of time-points\n",
    "which convey the most comfortable rate at which\n",
    "to tap along to the piece, and their\n",
    "metrical organisation, i.e., the grouping of\n",
    "beats into bars. The tempo can then be\n",
    "understood as the number of beats per minute,\n",
    "which is to say an indication as to whether\n",
    "the music is fast, meaning it has many \n",
    "beats per minute and hence a high tempo,\n",
    "or alternatively if the piece is slow\n",
    "and has a lower number of beats per minute. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb959cb6",
   "metadata": {},
   "source": [
    "Now let's look at the waveform again with the beats\n",
    "and downbeats drawn on top,\n",
    "and also listen to them rendered as short pulses. \n",
    "The higher pitched pulses\n",
    "correspond to the downbeat or the \"1\" of\n",
    "every bar, and the lower pitched pulses\n",
    "correspond to the remaining beats in each bar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b5562",
   "metadata": {},
   "outputs": [],
   "source": [
    "beats1 = np.loadtxt('../assets/ch2_basics/audio/easy_example.beats')\n",
    "downbeats1 = beats1[beats1[:, 1] == 1][:, 0]\n",
    "beats1 = beats1[:,0]\n",
    "\n",
    "y_beats1 = librosa.clicks(times=beats1, sr=SR, click_freq=1000.0, click_duration=0.1, click=None, length=len(x1))\n",
    "y_downbeats1 = librosa.clicks(times=downbeats1, sr=SR, click_freq=1500.0, click_duration=0.15, click=None, length=len(x1))\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "librosa.display.waveshow(x1, sr=SR, alpha=0.6)\n",
    "plt.vlines(beats1, 1.1*x1.min(), 1.1*x1.max(), label='Beats', color='r', linestyle=':', linewidth=2)\n",
    "plt.vlines(downbeats1, 1.1*x1.min(), 1.1*x1.max(), label='Downbeats', color='black', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.legend(fontsize=12); \n",
    "plt.title('Easy example: audio waveform with beats and downbeats', fontsize=15)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.xlabel('Time', fontsize=13)\n",
    "plt.xlim(0, len(x1)/sr);\n",
    "\n",
    "ipd.Audio(0.6*x1+0.25*y_beats1+0.25*y_downbeats1, rate=SR) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacaf568",
   "metadata": {},
   "source": [
    "Our intent is for this example to be relatively unambiguous in terms\n",
    "of the placement of the beats, and thus the \"preferred\" metrical level at which to tap. \n",
    "The beats are evenly spaced at a tempo of\n",
    "100 beats per minute (bpm) and with a constant 4/4 metre. The clear\n",
    "presence of drums with a repeating pattern of a kick drum events\n",
    "on the '1' and '3' and snare drum events on the '2' and '4'\n",
    "makes for a pretty straightforward excerpt both for human\n",
    "tappers and likewise for a computational system. \n",
    "In addition there are chord changes which also assist\n",
    "in understanding the harmonic rhythm of the piece. \n",
    "\n",
    "Having said that, outside of trivial cases (e.g., an isolated metronome), there \n",
    "is almost never likely to be complete agreement among tappers\n",
    "of the metrical level. Some people may prefer to tap at a slower\n",
    "rate (e.g., at half the tempo), while others may tap twice as fast.\n",
    "Alternatively some tappers might tap the so-called \"off-beat\"\n",
    "corresponding to the same tempo, but tapping in the mid-way point\n",
    "between each beat, i.e., shifted by one 1/8th note. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0f8d6a",
   "metadata": {},
   "source": [
    "## An expressive example\n",
    "\n",
    "Having looked at an easy example, let's transition \n",
    "to a more complicated case. As before, we'll begin\n",
    "by looking at the waveform and listening the audio excerpt.\n",
    "The piece in question is the Heitor Villa-Lobos composition Choros â„–1 as performed by [Kyuhee Park](https://www.youtube.com/watch?v=Uj_OferFIMk)\n",
    "\n",
    "\n",
    "\n",
    "Once again you can have a go tapping and counting along with the piece.\n",
    "\n",
    "```{note}\n",
    "Since the code examples are more or less identical for the remaining sound examples\n",
    "they're hidden by default, but clicking the plus symbol on the right hand side\n",
    "next to where it says \"Click to show\" will reveal them.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d968e8b9",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "x2, sr = librosa.load('../assets/ch2_basics/audio/expressive_example.flac', sr = SR)\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "plt.title('Expressive Example: audio waveform', fontsize=15)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.xlabel('Time', fontsize=13)\n",
    "plt.xlim(0, len(x2)/sr);\n",
    "librosa.display.waveshow(x2, sr=SR, alpha=0.6); \n",
    "\n",
    "ipd.Audio(x2, rate=SR) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9347bb71",
   "metadata": {},
   "source": [
    "**What's different?** \n",
    "* There are no drums, just a guitarist playing alone \n",
    "    * So no need to synchronise with anyone else\n",
    "    * In this sense there is no need for a \"shared sense\" of the beat\n",
    "* The tempo is **not** constant, and there are some big pauses to deal with!\n",
    "    * This makes it harder to predict the next beat - especially on the first listen\n",
    "    * Computationally, this also means that the tempo must be tracked through time as opposed to a single, global estimate  \n",
    "\n",
    "**What's the same?** \n",
    "* While the musical content is completely different, the goal is unchanged: \n",
    "    * Try to tap your foot in time with the beat and count the metrical positions\n",
    "* Thus, at a high level, the formulation of problem from a computational perspective is the same: \n",
    "    * audio-in, beats+dowbeats-out\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62cf597",
   "metadata": {},
   "source": [
    "Let's take a listen again, but this time with the beat and downbeat markers\n",
    "rendered as short pulses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d640e9c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "beats2 = np.loadtxt('../assets/ch2_basics/audio/expressive_example.beats')\n",
    "downbeats2 = beats2[beats2[:, 1] == 1][:, 0]\n",
    "beats2 = beats2[:,0]\n",
    "\n",
    "y_beats2 = librosa.clicks(times=beats2, sr=SR, click_freq=1000.0, click_duration=0.1, click=None, length=len(x2))\n",
    "y_downbeats2 = librosa.clicks(times=downbeats2, sr=SR, click_freq=1500.0, click_duration=0.15, click=None, length=len(x2))\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "librosa.display.waveshow(x2, sr=SR, alpha=0.6)\n",
    "plt.vlines(beats2, 1.1*x2.min(), 1.1*x2.max(), label='Beats', color='r', linestyle=':', linewidth=2)\n",
    "plt.vlines(downbeats2, 1.1*x2.min(), 1.1*x2.max(), label='Downbeats', color='black', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.legend(fontsize=12); \n",
    "plt.title('Expressive Example: audio waveform with beats and downbeats', fontsize=15)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.xlabel('Time', fontsize=13)\n",
    "plt.legend(fontsize=12); \n",
    "plt.xlim(0, len(x2)/sr);\n",
    "\n",
    "ipd.Audio(0.6*x2+0.25*y_beats2+0.25*y_downbeats2, rate=SR) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53008227",
   "metadata": {},
   "source": [
    "* Do you agree with the annotations? \n",
    "* Did you prefer to tap at a faster or slower rate?\n",
    "* Maybe you kept this tempo, but counted the beats in groups of four rather than two? \n",
    "* Maybe you felt the timing of the taps was a little off? Especially on the first beat!\n",
    "* Wait, why are there no beats in the first 5 seconds of the piece? \n",
    "\n",
    "In order to try to address these points we need to consider how this annotation\n",
    "was made. We'll see more about \"how to annotate\" in the next section, but for now\n",
    "we can make some headway in addressing these points by looking at a score representation\n",
    "for the very beginning of the piece."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7338f7c",
   "metadata": {},
   "source": [
    "```{figure} ../assets/ch2_basics/figs/choro_score.png\n",
    "---\n",
    "alt: Score representation of expressive excerpt\n",
    "width: 450px\n",
    "align: center\n",
    "name: choro\n",
    "---\n",
    "Score representation of expressive excerpt. Image taken from {cite}`pinto2021user`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd29792",
   "metadata": {},
   "source": [
    "Without wishing to get too technical in musical terms, \n",
    "the score shows that the first three notes of the piece, \n",
    "corresponding to its main *motif*, \n",
    "form an *anacrusis* - meaning a \"lead-in\" to the first \n",
    "complete bar, and are notated to be played in a very expressive manner\n",
    "at the discretion of the performer. \n",
    "While these three notes take up almost the first\n",
    "5 seconds of the piece, from a music notation perspective\n",
    "they are all 1/16th notes, and thus all fall **within** a single notated beat (i.e., one 1/4 note).\n",
    "On this basis, if we are to rely on the score, then \n",
    "none of the notes should be tapped at the start, nor\n",
    "when the motif repeats around the 30s mark.\n",
    "\n",
    "The score also tells us the time-signature of the piece is 2/4 and thus\n",
    "we should count the beats in groups of 2 rather than 4. \n",
    "This also lets us know which notes correspond to the beat-level \n",
    "and thus the rate at which we should tap (again, if we want to stick to the score).\n",
    "\n",
    "Finally, concerning the beginning of the first complete bar,\n",
    "i.e., the point at which the first overlaid pulse can be heard, \n",
    "this corresponds to an arpeggiated chord, where the plucking\n",
    "of the strings is not simultaneous, but drawn out so the\n",
    "individual notes of the chord can be heard.\n",
    "Again, notationally these notes correspond to a single\n",
    "beat, but this elongation of the chord makes it difficult to precisely\n",
    "identify a single time instant to mark as the beat.  \n",
    "\n",
    "\n",
    "In summary, if we rely on the score then this can \n",
    "help guide how the annotation should be performed.\n",
    "On the other hand, tapping purely based on listener\n",
    "perception, especially without prior familiarity\n",
    "with the peice, could lead to different annotations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abec4bd3",
   "metadata": {},
   "source": [
    "## A nonwestern example\n",
    "\n",
    "In our final example let's move away from \"western\" music and consider\n",
    "an excerpt of Uruguayan Candombe (taken from the [Candombe Recordings Dataset](http://www.eumus.edu.uy/candombe/datasets/ISMIR2015/). As before, please also have a go\n",
    "tapping and counting along!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936457ed",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "x3, sr = librosa.load('../assets/ch2_basics/audio/nonwestern_example.flac', sr = SR)\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "librosa.display.waveshow(x3, sr=SR, alpha=0.6); \n",
    "plt.title('Nonwestern Example: audio waveform', fontsize=15)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.xlim(0, len(x3)/sr);\n",
    "plt.xlabel('Time', fontsize=13)\n",
    "ipd.Audio(x3, rate=SR) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a53cab9",
   "metadata": {},
   "source": [
    "Let's now listen to the expert annotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf7ccdc",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "beats3 = np.loadtxt('../assets/ch2_basics/audio/nonwestern_example.beats')\n",
    "downbeats3 = beats3[beats3[:, 1] == 1][:, 0]\n",
    "beats3 = beats3[:,0]\n",
    "\n",
    "y_beats3 = librosa.clicks(times=beats3, sr=SR, click_freq=1000.0, click_duration=0.1, click=None, length=len(x3))\n",
    "y_downbeats3 = librosa.clicks(times=downbeats3, sr=SR, click_freq=1500.0, click_duration=0.15, click=None, length=len(x3))\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "librosa.display.waveshow(x3, sr=SR, alpha=0.6)\n",
    "plt.vlines(beats3, -1, 1, label='Beats', color='r', linestyle=':', linewidth=2)\n",
    "plt.vlines(downbeats3, -1, 1, label='Downbeats', color='black', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.title('Nonwestern Example: audio waveform with beats and downbeats', fontsize=15)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.xlabel('Time', fontsize=13)\n",
    "plt.legend(fontsize=12);  \n",
    "plt.xlim(0, len(x3)/sr);\n",
    "ipd.Audio(0.6*x3+0.25*y_beats3+0.25*y_downbeats3, rate=SR) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de077f2",
   "metadata": {},
   "source": [
    "The specific challenge here lies not only in the rhythmic density\n",
    "of the performance, but in understanding the metrical organisation.\n",
    "For those familiar with Candombe this is a straightforward\n",
    "excerise and as such it can be tapped and counted in an unambiguous\n",
    "way, but to the untrained ear (or algorithm!) it can be much more \n",
    "challenging to understanding how to tap along. \n",
    "\n",
    "As we proceed through the remainder of this part of the tutorial\n",
    "we'll revisit these examples again when we look at a baseline\n",
    "approach and also explore the means by which algorithms\n",
    "can be evaluated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3936c8cd",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section we've looked at three different sound examples\n",
    "as a means to intuitively understand the nature, interestingness,\n",
    "and challenges of the tasks of tempo, beat, and downbeat estimation.\n",
    "\n",
    "While we've touched upon it here, \n",
    "we'll now move on to considering the process of annotation in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c267f03d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

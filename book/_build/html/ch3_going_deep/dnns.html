
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>On different DNN architectures &#8212; Tempo, Beat and Downbeat Estimation</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e2363ea40746bee74734a24ffefccd78.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Post-processing" href="postprocessing.html" />
    <link rel="prev" title="Deep learning approaches" href="overview.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Tempo, Beat and Downbeat Estimation</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Tempo, Beat, and Downbeat Estimation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ch1_intro/tutorial_structure.html">
   Tutorial structure and setup
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch1_intro/tutorial_scope.html">
   Tutorial scope and prerequisites
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Basics of tempo, beat, and downbeat
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ch2_basics/definition.html">
   Definition by sound example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch2_basics/annotation.html">
   How do we annotate?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch2_basics/baseline.html">
   Baseline approach
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch2_basics/evaluate.html">
   How do we evaluate?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch2_basics/perspectives.html">
   Perspectives
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Going deep: theoretical underpinnings
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="overview.html">
   Deep learning approaches
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   On different DNN architectures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="postprocessing.html">
   Post-processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="design_decisions.html">
   Design decisions for tempo, beat, and downbeat
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="table.html">
   Table of reference works
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Going deeper: practial examples
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ch4_going_deeper/building_tcn_models.html">
   Hands on!
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Discussion and conclusions
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ch5_discussion/open_challenges.html">
   Concluding remarks
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Resources
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ch6_resources/references.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch6_resources/acknowledgments.html">
   Acknowledgments
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch6_resources/authors.html">
   About the Authors
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/ch3_going_deep/dnns.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/TempoBeatDownbeat/tutorial"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/TempoBeatDownbeat/tutorial/issues/new?title=Issue%20on%20page%20%2Fch3_going_deep/dnns.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-bit-of-context">
   A bit of context
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-a-deep-net">
   What is a deep net?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multi-layer-perceptrons">
   Multi-layer perceptrons
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#convolutional-neural-networks">
   Convolutional Neural networks
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#recurrent-networks">
   Recurrent networks
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gated-recurrent-units">
   Gated recurrent units
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bi-directional-models">
   Bi-directional models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#temporal-convolutional-networks">
   Temporal Convolutional networks
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hybrid-architectures">
   Hybrid architectures
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-and-optimization">
   Learning and optimization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#activation-functions">
   Activation functions
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="on-different-dnn-architectures">
<span id="dnns"></span><h1>On different DNN architectures<a class="headerlink" href="#on-different-dnn-architectures" title="Permalink to this headline">¶</a></h1>
<p>Many different architectures have been explored, ranging from MLPs <span id="id1">[<a class="reference internal" href="../ch6_resources/references.html#id11">DBDR15</a>]</span>, CNNs <span id="id2">[<a class="reference internal" href="../ch6_resources/references.html#id60">DBDR16</a>, <a class="reference internal" href="../ch6_resources/references.html#id65">DBDR17</a>, <a class="reference internal" href="../ch6_resources/references.html#id64">DE16</a>, <a class="reference internal" href="../ch6_resources/references.html#id211">HG16</a>]</span>, RNNs <span id="id3">[<a class="reference internal" href="../ch6_resources/references.html#id63">BockKW14a</a>, <a class="reference internal" href="../ch6_resources/references.html#id204">KHCW15</a>, <a class="reference internal" href="../ch6_resources/references.html#id232">ZNY19</a>]</span>, Bi-LSTMs <span id="id4">[<a class="reference internal" href="../ch6_resources/references.html#id214">BockKW16</a>]</span>,
Bi-GRUs <span id="id5">[<a class="reference internal" href="../ch6_resources/references.html#id12">KBockDW16</a>]</span>, CRNNs <span id="id6">[<a class="reference internal" href="../ch6_resources/references.html#id31">FMC+19</a>, <a class="reference internal" href="../ch6_resources/references.html#id9">FMC+18</a>, <a class="reference internal" href="../ch6_resources/references.html#id224">VDWK17</a>]</span> and recently TCNs <span id="id7">[<a class="reference internal" href="../ch6_resources/references.html#id6">BockD20</a>, <a class="reference internal" href="../ch6_resources/references.html#id5">BockDK19</a>, <a class="reference internal" href="../ch6_resources/references.html#id4">MBock19</a>]</span>. But, what have we learned from all this different approaches? Which one
works better? What should we take into account when using a particular architecture? Some context and thoughts on this below.</p>
<div class="section" id="a-bit-of-context">
<h2>A bit of context<a class="headerlink" href="#a-bit-of-context" title="Permalink to this headline">¶</a></h2>
<p>DNNs were introduced in MIR applications motivated by their huge success in computer vision, and due to recent advances that allow for faster training and scalability <span id="id8">[<a class="reference internal" href="../ch6_resources/references.html#id221">GBC16</a>]</span>.
The inclusion of these models in MIR tasks has meant a considerable improvement in the performance of automatic systems, in particular tempo, beat and downbeat tracking ones, as can be seen from the MIREX campaigns <span id="id9">[<a class="reference internal" href="../ch6_resources/references.html#id231">JLL19</a>]</span>.
Moreover, the use of deep learning models presents other advantages over traditional machine learning methods used in MIR, i.e. they are flexible and adaptable across tasks. As an example, convolutional neural network based models from Computer Vision were adapted for
onset detection <span id="id10">[<a class="reference internal" href="../ch6_resources/references.html#id196">SchluterBock14</a>]</span>, and then for  segment boundary detection <span id="id11">[<a class="reference internal" href="../ch6_resources/references.html#id138">USchluterG14</a>]</span>. Furthermore, DNNs reduce —or allow to remove completely— the stage of hand-crafted feature design, by including the feature learning as part of the learning
process.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The adoption of DNNs exacerbated some issues related to the use of supervised learning models: <strong>their dependence on annotated data, the bias of the data itself and their lack of interpretability</strong>. Annotated data is an important bottleneck in MIR! Especially due to copyright issues, and because annotating a musical piece requires
expert knowledge and is thus expensive. Also, models will be biased depending on the dataset used, a problem that also occurs in other learning-based approaches. Besides, deep-learning based methods are usually less interpretable than
signal processing methods, making it a bit hard to predict the type of mistakes a DNN would do when presented with e.g. unseen music tracks or genres.</p>
</div>
</div>
<div class="section" id="what-is-a-deep-net">
<h2>What is a deep net?<a class="headerlink" href="#what-is-a-deep-net" title="Permalink to this headline">¶</a></h2>
<p>In general terms, a deep neural network consists of a composition of non-linear functions that acts as a function approximator <span class="math notranslate nohighlight">\(F_\omega: \mathbf{X} \rightarrow \mathbf{Y}\)</span>, for given input and output data <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span>.
The network is parametrized by its weights <span class="math notranslate nohighlight">\(\omega\)</span>, whose values are optimized so the estimated output <span class="math notranslate nohighlight">\(\hat{\mathbf{Y}}=F_\omega (\mathbf{X})\)</span> approximates the desired output <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> given an input <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p>
</div>
<div class="section" id="multi-layer-perceptrons">
<h2>Multi-layer perceptrons<a class="headerlink" href="#multi-layer-perceptrons" title="Permalink to this headline">¶</a></h2>
<p>Multi-layer perceptrons (MLPs) are the simple and basic modules of DNNs. They are also known as <em>fully-connected layers</em> or <em>dense layers</em>, and consist of a sequence of layers, each defined by an affine transformation composed with a non-linearity:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{y} = f(\mathbf{W}^T \mathbf{x} + \mathbf{b}),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^{d_{in}}\)</span> is the input, <span class="math notranslate nohighlight">\(\mathbf{y} \in \mathbb{R}^{d_{out}}\)</span> is the output, <span class="math notranslate nohighlight">\(\mathbf{b} \in \mathbb{R}^{d_{out}}\)</span> is called the <em>bias vector</em> and <span class="math notranslate nohighlight">\(\mathbf{W} \in \mathbb{R}^{d_{in} \times d_{out}}\)</span> is the weight matrix. <span class="math notranslate nohighlight">\(f()\)</span> is a non-linear activation function, which allows the model to learn non-linear
representations. Note that for multi-dimensional inputs, e.g. <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{d_1 \times d_2}\)</span>, the  input is flattened so <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^{d}\)</span> with <span class="math notranslate nohighlight">\(d = d_1 \times d_2\)</span>. These layers are usually used to map the input to another space where hopefully the problem (e.g. classification or regression) can be solved more easily.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>However, by definition, this type of layer is not shift or scale invariant,
meaning that when using this type of network for audio tasks, any small temporal or frequency shift needs dedicated parameters to be modelled, becoming very expensive and inconvenient when it comes to modelling music.</p>
</div>
<p>MLPs have been mainly used in early works before convolutional neural networks (CNNs) and recurrent neural networks (RNNs) became popular <span id="id12">[<a class="reference internal" href="../ch6_resources/references.html#id222">CFCS17</a>]</span>, and are now used in combination with those architectures, usually as the last layers of a model to map high dimensional intermediate representations
to the output space (e.g. classes), as discussed below.</p>
</div>
<div class="section" id="convolutional-neural-networks">
<h2>Convolutional Neural networks<a class="headerlink" href="#convolutional-neural-networks" title="Permalink to this headline">¶</a></h2>
<p>The main idea behind CNNs is to convolve their input with learnable kernels. Systems based on CNNs make the most of the capacity of such networks to learn invariant properties of the data while needing fewer parameters than other DNNs such as MLPs and being easier to train. Also, convolutions are suitable for retrieving changes in the input representations,
which are (usually) indicators of beat and/or downbeat positions (e.g. changes in harmonic content or spectral energy). Besides, CNNs have shown to be good high-level feature extractors in music <span id="id13">[<a class="reference internal" href="../ch6_resources/references.html#id65">DBDR17</a>]</span>, and are able to express complex relations.</p>
<p>CNNs can be designed to perform either 1-d or 2-d convolutions, or a combination of both. In the context of audio, in general 1-d convolutions are used in the temporal domain,
whereas 2-d convolutions are usually applied to exploit time-frequency related information. We will focus on models that perform 2-d convolutions. The output of a convolutional layer is usually called <em>feature map</em>.
In the context of audio applications, it is common to use CNN architectures combining convolutional and pooling layers. Pooling layers are used to down-sample feature maps between convolutional layers, so that deeper layers integrate larger extents of data.
The most widely used pooling operator in the context of audio is <em>max-pooling</em>, which samples —usually— non-overlapping patches by keeping the biggest value in that region.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The main disadvantage of
CNNs is their lack of long-term context, which restrict the musical context and interplay with temporal scales that could improve their performance. This can be improved by combining CNNs with RNNs <span id="id14">[<a class="reference internal" href="../ch6_resources/references.html#id8">FMR+19</a>, <a class="reference internal" href="../ch6_resources/references.html#id9">FMC+18</a>, <a class="reference internal" href="../ch6_resources/references.html#id224">VDWK17</a>]</span>.</p>
</div>
<p>A convolutional layer is given by the following expression:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Y}^j = f(\sum_{k=0} ^{K-1} \mathbf{W}^{kj}\: *\: \mathbf{X}^k + \mathbf{b}^j),
\]</div>
<p>where all <span class="math notranslate nohighlight">\(\mathbf{Y}^j\)</span>, <span class="math notranslate nohighlight">\(\mathbf{W}^{jk}\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{X}^k\)</span> are 2-d, <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> is the bias vector, <span class="math notranslate nohighlight">\(j\)</span> indicates the j-th output channel, and <span class="math notranslate nohighlight">\(k\)</span> indicates the k-th input channel.
The input is a tensor <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{T\times F \times d}\)</span>, where <span class="math notranslate nohighlight">\(T\)</span> and <span class="math notranslate nohighlight">\(F\)</span> refer to the temporal and spatial—usually frequency—axes, and <span class="math notranslate nohighlight">\(d\)</span> denotes a non-convolutional dimension or <em>channel</em>.
In most audio applications <span class="math notranslate nohighlight">\(d\)</span> usually equals one, though sometimes is used to encode multiple channels or multiple representations of the input (e.g. each channel is one representation).
Note that while <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> are 3-d arrays (with axes for height, width and channel), <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> is a 4-d array, so <span class="math notranslate nohighlight">\(\mathbf{W} \in \mathbb{R}^{h \times l 
\times d_{in} \times d_{out}}\)</span>, <span class="math notranslate nohighlight">\(h\)</span> and <span class="math notranslate nohighlight">\(l\)</span> being the dimensions of the convolution, and the 3rd and 4th dimensions account for the relation between input and output channels.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>CNNs are suitable to problems that have two characteristics <span id="id15">[<a class="reference internal" href="../ch6_resources/references.html#id226">McF18</a>]</span>:  statistically meaningful information tends to concentrate locally (e.g. within a window around an event),
and shift-invariance (e.g. in time or frequency) can be used to reduce model complexity by reusing kernels’ weights with multiple inputs.</p>
</div>
</div>
<div class="section" id="recurrent-networks">
<h2>Recurrent networks<a class="headerlink" href="#recurrent-networks" title="Permalink to this headline">¶</a></h2>
<p>Many approaches exploited recurrent neural networks given their suitability to process sequential data. In theory, recurrent architectures are flexible in terms of the temporal context they can model, which makes them appealing for music applications.
In practice there are some limitations on the amount of context they can effectively learn <span id="id16">[<a class="reference internal" href="../ch6_resources/references.html#id234">GTH19</a>]</span>, and although it is clear that they can learn close metrical levels such as beats and downbeats
<span id="id17">[<a class="reference internal" href="../ch6_resources/references.html#id214">BockKW16</a>]</span>, is not clear if they can successfully learn interrelationships between farther temporal scales in music.</p>
<p>Unlike CNNs which are effective at modelling fixed-length local interactions, <em>recurrent neural networks</em> (RNNs) are good in modelling variable-length long-term interactions. RNNs exploit recurrent connections since they are formulated as <span id="id18">[<a class="reference internal" href="../ch6_resources/references.html#id221">GBC16</a>]</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
       \mathbf{y}_t = f_{y}(\mathbf{W}_y\:\mathbf{h}_t + \mathbf{b}_y), \label{eq:rnn_def_1}\\
        \mathbf{h}_t = f_{h}(\mathbf{W}_h\:\mathbf{x}_t + \mathbf{U}\: \mathbf{h}_{t-1} + \mathbf{b}_h),\label{eq:rnn_def_2}
\end{align}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{h}_t\)</span> is a hidden <em>state vector</em> that stores information at time <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(f_y\)</span> and <span class="math notranslate nohighlight">\(f_h\)</span> are the non-linearities of the output and hidden state respectively, and <span class="math notranslate nohighlight">\(\mathbf{W}_y, \mathbf{W}_h\)</span> and <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> are matrices of trainable weights.
An RNN integrates information over time up to time step <span class="math notranslate nohighlight">\(t\)</span> to estimate the state vector <span class="math notranslate nohighlight">\(\mathbf{h}_t\)</span>, being suitable to model sequential data.
Note that learning the weights <span class="math notranslate nohighlight">\(\mathbf{W}_y, \mathbf{W}_h\)</span> and <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> in a RNN is challenging given the dependency of the gradient on the entire state sequence <span id="id19">[<a class="reference internal" href="../ch6_resources/references.html#id226">McF18</a>]</span>. In practice, <em>back-propagation through time</em> is used
<span id="id20">[<a class="reference internal" href="../ch6_resources/references.html#id149">W+90</a>]</span>, which consists in unrolling the equation above up to <span class="math notranslate nohighlight">\(k\)</span> time steps and applying standard back propagation. Given the accumulative effect of applying <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> when unrolling the equation above, the gradient values tend to either vanish or explode if <span class="math notranslate nohighlight">\(k\)</span> is too big, a problem known as the <em>vanishing and exploding gradient problem</em>. For that reason, in practice the value of <span class="math notranslate nohighlight">\(k\)</span> is limited to account for relatively short sequences.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The most commonly used variations of RNNs, that were designed to mitigate the vanishing/exploding problem of the gradient, include the addition of <em>gates</em> that control the flow of information through the network. The most popular ones in MIR applications are <em>long-short memory units</em> (LSTMs) <span id="id21">[<a class="reference internal" href="../ch6_resources/references.html#id153">HS97</a>]</span> and <em>gated recurrent units</em> (GRUs) <span id="id22">[<a class="reference internal" href="../ch6_resources/references.html#id14">CVMerrienboerG+14</a>]</span>. Since they show similar
empirical results, we only discuss GRUs below.</p>
</div>
</div>
<div class="section" id="gated-recurrent-units">
<h2>Gated recurrent units<a class="headerlink" href="#gated-recurrent-units" title="Permalink to this headline">¶</a></h2>
<p>In a GRU layer, the <em>gate</em> variables <span class="math notranslate nohighlight">\(\mathbf{r}_t\)</span> and <span class="math notranslate nohighlight">\(\mathbf{u}_t\)</span> —named as <em>reset</em> and <em>update</em> vectors— control the updates to the state vector <span class="math notranslate nohighlight">\(\mathbf{h}_t\)</span>, which is a combination of the previous state <span class="math notranslate nohighlight">\(\mathbf{h}_{t-1}\)</span> and a proposed next state
<span class="math notranslate nohighlight">\(\hat{\mathbf{h}}_t\)</span>. The equations that rule these updates are given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
        \mathbf{r}_t = f_g(\mathbf{W}_r \mathbf{x}_t + \mathbf{U}_r \mathbf{h}_{t-1} + \mathbf{b}_r), \label{eq:gru_def_1}\\
        \mathbf{u}_t = f_g(\mathbf{W}_u \mathbf{x}_t + \mathbf{U}_u \mathbf{h}_{t-1} + \mathbf{b}_u),\label{eq:gru_def_2}\\
        \hat{\mathbf{h}}_t = f_h(\mathbf{W}_h \mathbf{x}_t + \mathbf{U}_h (\mathbf{r}_t\odot \mathbf{h}_{t-1}) + \mathbf{b}_h), \label{eq:gru_def_3}\\
         \mathbf{h}_t = \mathbf{u}_t\odot \mathbf{h}_{t-1} + (1-\mathbf{u}_t)\odot \hat{\mathbf{h}}_t;\label{eq:gru_def_4}
\end{align}
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\odot\)</span> indicates the element-wise Hadamard product, <span class="math notranslate nohighlight">\(f_g\)</span> is the activation applied to the reset and update vectors, and <span class="math notranslate nohighlight">\(f_h\)</span> is the output activation. <span class="math notranslate nohighlight">\(\mathbf{W}_r, \mathbf{W}_u, \mathbf{W}_h \in \mathbb{R}^{d_{i-1}\times d_i}\)</span> are the input weights, <span class="math notranslate nohighlight">\(\mathbf{U}_r, 
\mathbf{U}_u, \mathbf{U}_h \in \mathbb{R}^{d_{i}\times d_i}\)</span> are the recurrent weights and <span class="math notranslate nohighlight">\(\mathbf{b}_r, \mathbf{b}_u, \mathbf{b}_h \in \mathbb{R}^{d_{i}}\)</span> are the biases. The activation functions <span class="math notranslate nohighlight">\(f_g\)</span> and <span class="math notranslate nohighlight">\(f_h\)</span> are typically sigmoid and tanh, since saturating functions
help to avoid exploding gradients in recurrent networks.</p>
<p>The GRU operates as follows: when <span class="math notranslate nohighlight">\(\mathbf{u}_t\)</span> is close to 1, the previous observation <span class="math notranslate nohighlight">\(\mathbf{h}_{t-1}\)</span> dominates in the equations above. When <span class="math notranslate nohighlight">\(\mathbf{u}_t\)</span> gets close to 0, depending on the value of <span class="math notranslate nohighlight">\(\mathbf{r}_t\)</span>, either a new state is updated with the
standard recurrent equation by <span class="math notranslate nohighlight">\(\hat{\mathbf{h}}_t = f(\mathbf{W}_h \mathbf{x}_t + \mathbf{v}_h \mathbf{h}_{t-1} + \mathbf{b}_h)\)</span>, if <span class="math notranslate nohighlight">\(\mathbf{r}_t=1\)</span>, or the state is <em>reset</em> as if the <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span> was the first observation in the sequence by
<span class="math notranslate nohighlight">\(\hat{\mathbf{h}}_t = f(\mathbf{W}_h \mathbf{x}_t + \mathbf{b}_h)\)</span>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The reset variables allow GRUs to successfully model long-term interactions, and perform comparably to LSTMs, but GRUs are simpler since LSTMs have three gate vectors and one extra <em>memory</em> gate. Empirical studies show that both networks perform comparably
while GRUs are faster to train <span id="id23">[<a class="reference internal" href="../ch6_resources/references.html#id16">GSKoutnik+16</a>, <a class="reference internal" href="../ch6_resources/references.html#id15">JZS15</a>]</span>.</p>
</div>
</div>
<div class="section" id="bi-directional-models">
<h2>Bi-directional models<a class="headerlink" href="#bi-directional-models" title="Permalink to this headline">¶</a></h2>
<p>GRUs and RNNs in general are designed to integrate information in one direction, e.g. in an audio application they integrate information forward in time. However, it can be beneficial to integrate information in both directions, and so has been the case for neural networks
in audio applications such as beat tracking <span id="id24">[<a class="reference internal" href="../ch6_resources/references.html#id215">BockS11</a>]</span> or environmental sound detection <span id="id25">[<a class="reference internal" href="../ch6_resources/references.html#id217">PHV16</a>]</span>. A bi-directional recurrent neural network (Bi-RNN) <span id="id26">[<a class="reference internal" href="../ch6_resources/references.html#id152">SP97</a>]</span> in the context of audio consists of two RNNs running
in opposite time directions with their hidden vectors <span class="math notranslate nohighlight">\(\mathbf{h}_t ^f\)</span> and <span class="math notranslate nohighlight">\(\mathbf{h}_t ^b\)</span> being concatenated, so the output <span class="math notranslate nohighlight">\(h_t\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> has information about the entire sequence. Unless the application is online, Bi-RNNs are usually preferred due to better performance
<span id="id27">[<a class="reference internal" href="../ch6_resources/references.html#id226">McF18</a>]</span>.</p>
</div>
<div class="section" id="temporal-convolutional-networks">
<h2>Temporal Convolutional networks<a class="headerlink" href="#temporal-convolutional-networks" title="Permalink to this headline">¶</a></h2>
<p>A “simple” convolution is only able to consider the context up to a size linear in the depth of the network. This
makes it challenging to apply them on sequential data, because the amount of context they can handle is small. In the
context of beat and downbeat tracking systems, that might mean that the input granularity should be coarser if we want to
guarantee that enough context is taken into account for e.g. downbeat tracking. Instead, Temporal Convolutional Networks (TCNs)
<span id="id28">[<a class="reference internal" href="../ch6_resources/references.html#id2">BKK18</a>]</span> use dilated convolutions which enable exponentially large receptive fields! Formally, for a 1-D sequence input <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^d_{in}\)</span> and a filter <span class="math notranslate nohighlight">\(f : \{0, \dots, k − 1\} \rightarrow \mathbb{R}\)</span>, the dilated convolution
operation <span class="math notranslate nohighlight">\(F\)</span> on element <span class="math notranslate nohighlight">\(s\)</span> of the sequence is defined as:</p>
<div class="math notranslate nohighlight">
\[
F(s) = \sum _{i=0} ^{k-1} f(i) x_{s−d·i} 
\]</div>
<p>where <span class="math notranslate nohighlight">\(d\)</span> is the dilation factor, <span class="math notranslate nohighlight">\(k\)</span> is the filter size, and <span class="math notranslate nohighlight">\(s − d ·i\)</span> accounts for the direction of the past.
Dilation is equivalent to introducing a fixed step between every two adjacent filter taps, i.e. skipping samples in the audio.
When <span class="math notranslate nohighlight">\(d = 1\)</span>, a dilated convolution becomes a regular convolution.</p>
<div class="figure align-default" id="tcn">
<img alt="TCN" src="../_images/tcn.png" />
<p class="caption"><span class="caption-number">Fig. 14 </span><span class="caption-text">Overview of the TCN structure from <span id="id29">[<a class="reference internal" href="../ch6_resources/references.html#id4">MBock19</a>]</span>.</span><a class="headerlink" href="#tcn" title="Permalink to this image">¶</a></p>
</div>
<p>Intuitively, TCNs perform convolutions across sub-sampled input representations and are good at learning sequential/temporal structure,
since they retain the parallelisation property of standard CNNs but can handle much more context. Besides, TCNs can be trained
far more efficiently than a RNN, LSTM or GRU from a computational perspective, and with much less number of weights. Given this
advantages, TCNs are taking over these recurrent networks in many sequential tasks.</p>
<div class="dropdown admonition">
<p class="admonition-title">SPOILER ALERT!!</p>
<p>Because of being light, fast to train and have great performance, we’re using TCNs for the hands on part of the tutorial!</p>
</div>
</div>
<div class="section" id="hybrid-architectures">
<h2>Hybrid architectures<a class="headerlink" href="#hybrid-architectures" title="Permalink to this headline">¶</a></h2>
<p>As mentioned before, MLPs are now usually being used in combination with CNNs, which are able to overcome the lack of shift and scale invariance MLPs suffer. At the same time, MLPs offer a simple alternative for mapping representations from a big-dimensional space to a
smaller one, suitable for classification problems.</p>
<p>Finally, hybrid architectures that integrate convolutional and recurrent networks have recently become popular and have proven to be effective in audio applications, especially in MIR <span id="id30">[<a class="reference internal" href="../ch6_resources/references.html#id118">MB17</a>, <a class="reference internal" href="../ch6_resources/references.html#id220">SBD16</a>]</span>. They integrate local feature learning with
global feature integration, a playground between time scales that is in particular interesting for beat and downbeat tracking.</p>
</div>
<div class="section" id="learning-and-optimization">
<h2>Learning and optimization<a class="headerlink" href="#learning-and-optimization" title="Permalink to this headline">¶</a></h2>
<p>To optimize the parameters <span class="math notranslate nohighlight">\(\omega\)</span>, a variant of gradient descent is usually exploited. A <em>loss function</em> <span class="math notranslate nohighlight">\(J(\omega)\)</span> measures the difference between the predicted and desired outputs <span class="math notranslate nohighlight">\(\hat{\mathbf{Y}}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span>, so the main idea behind the optimization process is
to iteratively update the weights <span class="math notranslate nohighlight">\(\omega\)</span> so the loss function decreases, that is:</p>
<div class="math notranslate nohighlight">
\[
\omega \leftarrow \omega - \eta \nabla_\omega J(\omega).
\]</div>
<p>Where <span class="math notranslate nohighlight">\(\eta\)</span> is the <em>learning rate</em> which controls how much to update the values of <span class="math notranslate nohighlight">\(\omega\)</span> at each iteration. Because the DNN consists of a composition of functions, the gradient of <span class="math notranslate nohighlight">\(J(\omega)\)</span>, <span class="math notranslate nohighlight">\(\nabla \omega J(\omega)\)</span>, is obtained via the chain rule, a process known as
<em>back propagation</em>. In the last four years, many software packages that implement automatic differentiation tools and various versions of gradient descent were released, <span id="id31">[<a class="reference internal" href="../ch6_resources/references.html#id206">AAB+15</a>, <a class="reference internal" href="../ch6_resources/references.html#id143">C+15</a>, <a class="reference internal" href="../ch6_resources/references.html#id218">TheanoDTeam16</a>]</span>, reducing considerably the time needed for the
implementation of such models.</p>
<p>Since computing the gradient over a large training set is very expensive both in memory and computational complexity, a widely adopted variant of gradient descent is <em>Stochastic Gradient Descent</em> (SGD) <span id="id32">[<a class="reference internal" href="../ch6_resources/references.html#id150">Bot91</a>]</span>, which approximates the gradient at each
step on a mini-batch of training samples, <span class="math notranslate nohighlight">\(B\)</span>, considerably smaller than the training set. There are other variants of SGD such as <em>momentum</em> methods or <em>adaptive update</em> schemes that accelerate convergence dramatically, by re-using information of previous gradients
(momentum) and reducing the dependence on <span class="math notranslate nohighlight">\(\eta\)</span>. From 2017, the most popular method for optimizing DNNs has been the adaptive method ADAM <span id="id33">[<a class="reference internal" href="../ch6_resources/references.html#id17">KB14</a>]</span>.</p>
<p>Another common practice in the optimization of DNNs is to use <em>early stopping</em> as regularization <span id="id34">[<a class="reference internal" href="../ch6_resources/references.html#id151">SjobergL95</a>]</span>, which means to stop training if the training –or validation– loss is not improving after a certain amount of iterations. Finally,
<em>batch normalization</em> (BN) <span id="id35">[<a class="reference internal" href="../ch6_resources/references.html#id20">IS15</a>]</span> is widely used in practice as well, and consists of scaling the data by estimating its statistics during training, which usually leads to better performance and faster convergence.</p>
</div>
<div class="section" id="activation-functions">
<h2>Activation functions<a class="headerlink" href="#activation-functions" title="Permalink to this headline">¶</a></h2>
<p>The expressive power of DNNs is in great extent due to the use of non-linearities <span class="math notranslate nohighlight">\(f()\)</span> in the model. The type of non-linearity used depends on whether it is an internal layer or the output layer. Many different options have been explored in the literature for
intermediate-layer non-linearities —usually named <em>transfer functions</em>, the two main groups being saturating or non-saturating functions (e.g. <em>tanh</em> or <em>sigmoid</em> for saturated, because they saturate in 0 and 1, and <em>rectified linear units</em> (ReLUs)
<span id="id36">[<a class="reference internal" href="../ch6_resources/references.html#id179">NH10</a>]</span> for non-saturating ones). Usually non-saturating activations are preferred in practice for being simpler to train and increasing training speed <span id="id37">[<a class="reference internal" href="../ch6_resources/references.html#id226">McF18</a>]</span>.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./ch3_going_deep"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="overview.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Deep learning approaches</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="postprocessing.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Post-processing</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Matthew E. P. Davies, Sebastian Bock, Magdalena Fuentes<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>
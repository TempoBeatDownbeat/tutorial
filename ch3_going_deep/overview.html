
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Deep learning approaches &#8212; Tempo, Beat and Downbeat Estimation</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e2363ea40746bee74734a24ffefccd78.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="On different DNN architectures" href="dnns.html" />
    <link rel="prev" title="Perspectives" href="../ch2_basics/perspectives.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Tempo, Beat and Downbeat Estimation</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Tempo, Beat, and Downbeat Estimation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ch1_intro/tutorial_structure.html">
   Tutorial structure and setup
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch1_intro/tutorial_scope.html">
   Tutorial scope and prerequisites
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Basics of tempo, beat, and downbeat
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ch2_basics/definition.html">
   Definition by sound example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch2_basics/annotation.html">
   How do we annotate?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch2_basics/baseline.html">
   Baseline approach
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch2_basics/evaluate.html">
   How do we evaluate?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch2_basics/perspectives.html">
   Perspectives
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Going deep: theoretical underpinnings
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Deep learning approaches
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dnns.html">
   On different DNN architectures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="postprocessing.html">
   Post-processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="design_decisions.html">
   Design decisions for tempo, beat, and downbeat
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="table.html">
   Table of reference works
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Going deeper: practial examples
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ch4_going_deeper/building_tcn_models.html">
   Hands on!
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Discussion and conclusions
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ch5_discussion/open_challenges.html">
   Concluding remarks
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Resources
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ch6_resources/references.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch6_resources/acknowledgments.html">
   Acknowledgments
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch6_resources/authors.html">
   About the Authors
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/ch3_going_deep/overview.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/TempoBeatDownbeat/tutorial"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/TempoBeatDownbeat/tutorial/issues/new?title=Issue%20on%20page%20%2Fch3_going_deep/overview.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#feature-extraction">
   Feature extraction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#likelihood-estimation">
   Likelihood estimation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inference">
   Inference
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#next">
   Next
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="deep-learning-approaches">
<span id="dnns-overview"></span><h1>Deep learning approaches<a class="headerlink" href="#deep-learning-approaches" title="Permalink to this headline">¶</a></h1>
<p>The goal of these sections is to provide the theoretical underpinnings of deep learning models or <em>deep neural networks</em> (DNNs) commonly used for tempo, beat and downbeat tracking, to build an understanding of why these models work well in these tasks and what are their limitations. Here we discuss the main concepts and definitions,
but for those interested in reading more about the use of DNNs in audio processing and MIR related tasks, we suggest them to take a look at works like McFee <span id="id1">[<a class="reference internal" href="../ch6_resources/references.html#id225">McF18</a>]</span>, Purwins et al. <span id="id2">[<a class="reference internal" href="../ch6_resources/references.html#id227">PLV+19</a>]</span> and Choi et al. <span id="id3">[<a class="reference internal" href="../ch6_resources/references.html#id221">CFCS17</a>]</span>.</p>
<p>The field moved quite fast these past years and the amount different design choices and approaches can be somehow overwhelming!
We tried to come up with a way of summarizing the different moving parts of beat and downbeat deep learning systems to help
digest this, as we explain below.</p>
<p>Recent beat and downbeat tracking approaches can be structured in three main stages: 1) A first stage of <em>low-level feature</em> computation or <em>feature extraction</em>, where feature vectors that represent the content of musical audio are
extracted from the raw audio signal (e.g. Spectrogram, Chromagram); 2) A second step that usually consists of a stage of feature learning, whose outcome is an activation function that indicates the most likely candidates
for beats and/or downbeats among the input audio observations; 3) Finally, a post-processing stage is often used, usually consisting of a probabilistic graphical model which encodes some relevant musical rules
to select the final beat/downbeat candidates.</p>
<div class="figure align-default" id="id25">
<img alt="General pipeline commonly used for beat and/or downbeat tracking systems." src="../_images/diagram.png" />
<p class="caption"><span class="caption-number">Fig. 10 </span><span class="caption-text">General pipeline commonly used for beat and/or downbeat tracking systems.</span><a class="headerlink" href="#id25" title="Permalink to this image">¶</a></p>
</div>
<p>Different alternatives were proposed for the distinct stages among beat and downbeat tracking systems. Here we give an overview of the main ideas
presented in the literature.</p>
<div class="section" id="feature-extraction">
<h2>Feature extraction<a class="headerlink" href="#feature-extraction" title="Permalink to this headline">¶</a></h2>
<p>It is common to exploit music knowledge for feature design using signal processing techniques.
The three most explored categories of musically inspired features in the literature for both beat and downbeat estimation are: chroma (CH) <span id="id4">[<a class="reference internal" href="../ch6_resources/references.html#id30">FMC+19</a>, <a class="reference internal" href="../ch6_resources/references.html#id8">FMC+18</a>, <a class="reference internal" href="../ch6_resources/references.html#id187">HDF12</a>, <a class="reference internal" href="../ch6_resources/references.html#id188">KFRO12</a>, <a class="reference internal" href="../ch6_resources/references.html#id11">KBockDW16</a>, <a class="reference internal" href="../ch6_resources/references.html#id176">PP10a</a>, <a class="reference internal" href="../ch6_resources/references.html#id179">PP10b</a>]</span> —used to reflect the harmonic content of the signal—,
onset detection function (ODF) <span id="id5">[<a class="reference internal" href="../ch6_resources/references.html#id10">DBDR15</a>, <a class="reference internal" href="../ch6_resources/references.html#id187">HDF12</a>, <a class="reference internal" href="../ch6_resources/references.html#id202">ZDGomez14</a>]</span> or spectral flux (SF) <span id="id6">[<a class="reference internal" href="../ch6_resources/references.html#id30">FMC+19</a>, <a class="reference internal" href="../ch6_resources/references.html#id8">FMC+18</a>, <a class="reference internal" href="../ch6_resources/references.html#id32">HKS14</a>, <a class="reference internal" href="../ch6_resources/references.html#id188">KFRO12</a>, <a class="reference internal" href="../ch6_resources/references.html#id190">KBockW13</a>]</span> —as event-oriented indicators— and timbre inspired features <span id="id7">[<a class="reference internal" href="../ch6_resources/references.html#id200">DDR14</a>, <a class="reference internal" href="../ch6_resources/references.html#id187">HDF12</a>, <a class="reference internal" href="../ch6_resources/references.html#id194">SHS14</a>]</span> such as spectral coefficients or MFCCs.
For beat, the main features exploited are those related to event-oriented indicators, assuming that changes in the spectral energy relate to
beat positions are <span id="id8">[<a class="reference internal" href="../ch6_resources/references.html#id180">DRuaP+12</a>, <a class="reference internal" href="../ch6_resources/references.html#id28">FJDE15</a>, <a class="reference internal" href="../ch6_resources/references.html#id32">HKS14</a>, <a class="reference internal" href="../ch6_resources/references.html#id190">KBockW13</a>, <a class="reference internal" href="../ch6_resources/references.html#id203">KHCW15</a>, <a class="reference internal" href="../ch6_resources/references.html#id104">NRJB15</a>, <a class="reference internal" href="../ch6_resources/references.html#id51">SHCS15</a>]</span>. For downbeat, harmonic-related features showed to be relevant to estimate downbeats reliably across music genres.</p>
<p>The feature extraction is usually based on a single feature <span id="id9">[<a class="reference internal" href="../ch6_resources/references.html#id62">BockKW14a</a>, <a class="reference internal" href="../ch6_resources/references.html#id32">HKS14</a>, <a class="reference internal" href="../ch6_resources/references.html#id201">KBockW14</a>, <a class="reference internal" href="../ch6_resources/references.html#id18">KBockW11</a>, <a class="reference internal" href="../ch6_resources/references.html#id176">PP10a</a>, <a class="reference internal" href="../ch6_resources/references.html#id179">PP10b</a>, <a class="reference internal" href="../ch6_resources/references.html#id51">SHCS15</a>, <a class="reference internal" href="../ch6_resources/references.html#id202">ZDGomez14</a>]</span>, with some exceptions exploiting more than one music property at the same time <span id="id10">[<a class="reference internal" href="../ch6_resources/references.html#id10">DBDR15</a>, <a class="reference internal" href="../ch6_resources/references.html#id59">DBDR16</a>, <a class="reference internal" href="../ch6_resources/references.html#id64">DBDR17</a>, <a class="reference internal" href="../ch6_resources/references.html#id30">FMC+19</a>, <a class="reference internal" href="../ch6_resources/references.html#id8">FMC+18</a>, <a class="reference internal" href="../ch6_resources/references.html#id11">KBockDW16</a>, <a class="reference internal" href="../ch6_resources/references.html#id202">ZDGomez14</a>]</span>, which results in systems robust to different music genres <span id="id11">[<a class="reference internal" href="../ch6_resources/references.html#id64">DBDR17</a>]</span>.
Recently, approaches based on deep learning exploring combinations of logarithmic spectrograms with different resolutions showed to perform competently <span id="id12">[<a class="reference internal" href="../ch6_resources/references.html#id5">BockD20</a>, <a class="reference internal" href="../ch6_resources/references.html#id4">BockDK19</a>, <a class="reference internal" href="../ch6_resources/references.html#id213">BockKW16</a>, <a class="reference internal" href="../ch6_resources/references.html#id201">KBockW14</a>]</span>.</p>
<div class="figure align-default" id="id26">
<img alt="Example of features used for downbeat tracking." src="../_images/features_example.png" />
<p class="caption"><span class="caption-number">Fig. 11 </span><span class="caption-text">Example of features, from left to right: melodic constant-Q transform, onset detection function, chromagram, low-frequency spectrogram. Adapted from <span id="id13">[<a class="reference internal" href="../ch6_resources/references.html#id64">DBDR17</a>]</span>.</span><a class="headerlink" href="#id26" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="likelihood-estimation">
<h2>Likelihood estimation<a class="headerlink" href="#likelihood-estimation" title="Permalink to this headline">¶</a></h2>
<p>The objective of this stage is to map the input representation into a beat/downbeat likelihood that indicates which are the most likely candidates to be a beat or a downbeat
in a given temporal sequence. There are two main groups of approaches in this respect: the first one uses “heuristics” to perform the mapping, while the second
group exploits machine learning approaches. The latter group is the most popular one in the literature in the last years and also the state of the art.</p>
<p>The estimation of a likelihood with heuristics is performed differently depending on the features used. For instance, a common approach is to pre-define a template of <em>expected</em> features such as
spectral-flux or chroma, and to measure the distance between this template to the features computed from the audio signal <span id="id14">[<a class="reference internal" href="../ch6_resources/references.html#id104">NRJB15</a>, <a class="reference internal" href="../ch6_resources/references.html#id179">PP10b</a>]</span>. Within the group of
machine learning approaches, we could identify two subgroups: a first one that exploits “traditional” learning techniques and a second one with focus on deep learning models.</p>
<div class="figure align-default" id="id27">
<img alt="Example of rhythmic patter learning." src="../_images/rhythmic_patterns.png" />
<p class="caption"><span class="caption-number">Fig. 12 </span><span class="caption-text">Example of rhythmic patter learning from <span id="id15">[<a class="reference internal" href="../ch6_resources/references.html#id190">KBockW13</a>]</span>.</span><a class="headerlink" href="#id27" title="Permalink to this image">¶</a></p>
</div>
<p>Before deep learning, machine learning systems often focus on recognizing rhythm patterns in data, for instance by using <em>Gaussian Mixture Models</em> (GMM) and k-means
<span id="id16">[<a class="reference internal" href="../ch6_resources/references.html#id32">HKS14</a>, <a class="reference internal" href="../ch6_resources/references.html#id190">KBockW13</a>, <a class="reference internal" href="../ch6_resources/references.html#id203">KHCW15</a>, <a class="reference internal" href="../ch6_resources/references.html#id104">NRJB15</a>, <a class="reference internal" href="../ch6_resources/references.html#id51">SHCS15</a>, <a class="reference internal" href="../ch6_resources/references.html#id46">SHCS16</a>, <a class="reference internal" href="../ch6_resources/references.html#id224">SHS17</a>]</span>. This usually required making some assumptions of
style or genre (e.g. to define the length of the patterns to be learned), and for these models to be effective the music should have distinctive rhythmic patterns.
Deep learning approaches propose an alternative to such limitations given their capacity to learn complex function mappings, and systems exploiting DNNs have became the state of the art in
recent years <span id="id17">[<a class="reference internal" href="../ch6_resources/references.html#id230">JLL19</a>]</span>.</p>
<div class="figure align-default" id="id28">
<img alt="Example of likelihood estimation." src="../_images/feature_extraction.png" />
<p class="caption"><span class="caption-number">Fig. 13 </span><span class="caption-text">Different stages of feature extraction. Left: input spectrogram, middle: intermediate DNN outputs, right: the final beat and downbeat likelihoods. Adapted from <span id="id18">[<a class="reference internal" href="../ch6_resources/references.html#id213">BockKW16</a>]</span>.</span><a class="headerlink" href="#id28" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="inference">
<h2>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">¶</a></h2>
<p>The aim of this stage is to obtain the final downbeat sequence by selecting the most likely candidates in the downbeat likelihood given some model or criteria. Probabilistic graphical models (PGMs) are the most used
post-processing techniques since 2010. This might be due to two main reasons: PGMs offer a flexible framework to incorporate music knowledge and then exploit interrelated structure <span id="id19">[<a class="reference internal" href="../ch6_resources/references.html#id176">PP10a</a>, <a class="reference internal" href="../ch6_resources/references.html#id179">PP10b</a>]</span>, and the
Bar Pointer Model (BPM) <span id="id20">[<a class="reference internal" href="../ch6_resources/references.html#id54">WCG06</a>]</span> (<strong>ADD INTERNAL REFERENCE</strong>) stands as a very effective and adaptable model for meter tracking, being popular for beat and downbeat tracking.</p>
<p>PGMs proved to be adaptable to cultural-aware systems in diverse music cultures <span id="id21">[<a class="reference internal" href="../ch6_resources/references.html#id32">HKS14</a>, <a class="reference internal" href="../ch6_resources/references.html#id104">NRJB15</a>, <a class="reference internal" href="../ch6_resources/references.html#id194">SHS14</a>]</span>, being for instance extendable to track longer meter cycles and different meters than the widely explored 3/4 and 4/4 <span id="id22">[<a class="reference internal" href="../ch6_resources/references.html#id46">SHCS16</a>, <a class="reference internal" href="../ch6_resources/references.html#id224">SHS17</a>]</span>.
Considerable efforts have been made towards improving the use of these models in practice, by reducing computational cost via an efficient state-space definition <span id="id23">[<a class="reference internal" href="../ch6_resources/references.html#id18">KBockW11</a>]</span> or proposing <em>sequential Monte Carlo</em> methods (also called <em>particle filters</em>) for inference <span id="id24">[<a class="reference internal" href="../ch6_resources/references.html#id203">KHCW15</a>, <a class="reference internal" href="../ch6_resources/references.html#id51">SHCS15</a>]</span>.</p>
</div>
<div class="section" id="next">
<h2>Next<a class="headerlink" href="#next" title="Permalink to this headline">¶</a></h2>
<p>After discussing the usual pipelines of beat and downbeat tracking systems, let’s dig a bit more in the different deep learning architectures used for likelihood estimation these past years.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./ch3_going_deep"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="../ch2_basics/perspectives.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Perspectives</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="dnns.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">On different DNN architectures</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Matthew E. P. Davies, Sebastian Bock, Magdalena Fuentes<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>